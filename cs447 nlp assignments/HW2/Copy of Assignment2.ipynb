{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Assignment2.ipynb","provenance":[{"file_id":"1hlplW7iMfmdphSOpUwfwzfgMsdlpnlux","timestamp":1571081815759}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"pSy-sfxOsclS","colab_type":"text"},"source":["# Assignment 2"]},{"cell_type":"markdown","metadata":{"id":"aIc7zIy7qrIt","colab_type":"text"},"source":["In this part of assignment 2 we'll be building a machine learning model to detect sentiment of movie reviews using the Stanford Sentiment Treebank([SST])(http://ai.stanford.edu/~amaas/data/sentiment/) dataset. First we will import all the required libraries. We highly recommend that you finish the PyTorch Tutorials [ 1 ](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html),[ 2 ](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html),[ 3 ](https://github.com/yunjey/pytorch-tutorial). before starting this assignment. After finishing this assignment we will able to answer the following questions-\n","\n","\n","* How to write Dataloaders in Pytorch?\n","* How to build dictionaries and vocabularies for Deep Nets?\n","* How to use Embedding Layers in Pytorch?\n","* How to build various recurrent models (LSTMs and GRUs) for sentiment analysis?\n","* How to use packed_padded_sequences for sequential models?\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jgzwGooD2bpy","colab_type":"text"},"source":["# Import Libraries"]},{"cell_type":"code","metadata":{"id":"EyCOvTRQ1nb-","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","from collections import defaultdict\n","from torchtext import datasets\n","from torchtext import data\n","\n","from torch.nn.utils.rnn import pack_sequence, pad_sequence"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zHbJ1-aDsWCG","colab_type":"text"},"source":["## Download dataset\n","First we will download the dataset using [torchtext](https://torchtext.readthedocs.io/en/latest/index.html), which is a package that supports NLP for PyTorch. The following command will get you 3 objects `train_data`, `val_data` and `test_data`. To access the data:\n","\n","*   To access list of textual tokens - `train_data[0].text`\n","*   To access label - `train_data[0].label`\n","\n"]},{"cell_type":"code","metadata":{"id":"dfX3bNby8FYL","colab_type":"code","outputId":"8a771a85-6065-4814-de5a-fdddd0089e20","executionInfo":{"status":"ok","timestamp":1575660699316,"user_tz":480,"elapsed":16253,"user":{"displayName":"Joe Fetsch","photoUrl":"","userId":"03543581272752113099"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["if(__name__=='__main__'):\n","  train_data, val_data, test_data = datasets.SST.splits(data.Field(tokenize = 'spacy'), data.LabelField(dtype = torch.float), filter_pred=lambda ex: ex.label != 'neutral')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["downloading trainDevTestTrees_PTB.zip\n"],"name":"stdout"},{"output_type":"stream","text":["trainDevTestTrees_PTB.zip: 100%|██████████| 790k/790k [00:01<00:00, 437kB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["extracting\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xCabYGV5x22f","colab_type":"code","outputId":"dee060ef-fbb1-432b-8577-5a98dc235b82","executionInfo":{"status":"ok","timestamp":1575660699317,"user_tz":480,"elapsed":16242,"user":{"displayName":"Joe Fetsch","photoUrl":"","userId":"03543581272752113099"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["if(__name__=='__main__'):\n","  print(train_data[0].text)\n","  print(train_data[0].label)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '`', '`', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean', '-', 'Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.']\n","positive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_kfg8RcyskyU","colab_type":"text"},"source":["## Define the Dataset Class\n","\n","In the following cell, we will define the dataset class. You need to implement the following functions: \n","\n","\n","*   ` build_dictionary() ` - creates the dictionaries `ixtoword` and `wordtoix`. Converts all the text of all examples, in the form of text ids and stores them in `textual_ids`. If a word is not present in your dictionary, it should use `<unk>`. Use the hyperparameter `THRESHOLD` to control the words to be in the dictionary based on their occurrence. Note the occurrences should be `>=THRESHOLD` to be included in the dictionary.\n","*   ` get_label() ` - It should return the value `0` if the label in the dataset is `positive`, and should return `1` if it is `negative`. \n","*   ` get_text() ` - This function should pad the review with `<end>` character uptil a length of `MAX_LEN` if the length of the text is less than the `MAX_LEN`.\n","*   ` __len__() ` - This function should return the total length of the dataset.\n","*   ` __getitem__() ` - This function should return the padded text, the length of the text (without the padding) and the label.\n"]},{"cell_type":"code","metadata":{"id":"1irMn3LX2YDB","colab_type":"code","colab":{}},"source":["THRESHOLD = 10\n","MAX_LEN = 60\n","INPUT_DIM = 0\n","BATCH_SIZE = 32\n","UNK = 1\n","END = 0\n","class TextDataset(data.Dataset):\n","  def __init__(self, examples, split, ixtoword=None, wordtoix=None, THRESHOLD=THRESHOLD):\n","    self.examples = examples\n","    self.split = split\n","    self.THRESHOLD = THRESHOLD\n","    self.textual_ids, self.textual_labels, self.ixtoword, self.wordtoix = self.build_dictionary(wordtoix, ixtoword)\n","    print(\"Built dicts with len =\",len(self.wordtoix))\n","    print(\"  from source with\",len(self.examples),\"reviews\")\n","    print(\"  producing\",len(self.textual_ids),\"index vectors\")\n","    print(\"  with\",self.positives,\"positive reviews and\",len(self.textual_ids)-self.positives,\"negative ones\")\n","    print(\"  at Threshold =\",THRESHOLD)\n","  \n","  def build_dictionary(self, wordtoix1=None, ixtoword1=None):\n","    global INPUT_DIM\n","    # do count checks for threshold comparisons\n","    wordCounts = {}\n","    for example in self.examples:\n","      for token in example.text:\n","        token = token.lower()\n","        if token not in wordCounts:\n","          wordCounts[token] = 1\n","        else:\n","          wordCounts[token] += 1\n","    ixtoword=ixtoword1\n","    wordtoix=wordtoix1\n","    \n","    textual_ids = []\n","    textual_labels = []\n","\n","    if wordtoix==None and ixtoword==None:\n","      # create indices for all entries with count >= Threshold\n","      ixtoword = {}\n","      wordtoix = {}\n","      nextIndex = 2\n","    \n","      ### <end> should be at idx 0\n","      ### <unk> should be at idx 1 \n","      ixtoword[END] = \"<end>\"\n","      wordtoix[\"<end>\"] = END\n","      ixtoword[UNK] = \"<unk>\"\n","      wordtoix[\"<unk>\"] = UNK\n","    \n","      # index mappings complete: usable to determine <unk>\n","      for word in wordCounts.keys():\n","        if wordCounts[word] >= self.THRESHOLD:\n","          ixtoword[nextIndex] = word\n","          wordtoix[word] = nextIndex\n","          nextIndex += 1\n","    \n","    self.positives = 0\n","    \n","    # create textual_ids\n","    for index in range(len(self.examples)):\n","      sentence = []\n","      for token in self.examples[index].text:\n","        if token in wordtoix.keys():\n","          sentence.append(wordtoix[token])\n","        else:\n","          sentence.append(UNK)\n","          \n","      textual_ids.append(sentence)\n","      textual_labels.append(self.examples[index].label)\n","      self.positives += (1-(self.examples[index].label == 'positive'))\n","      \n","    INPUT_DIM = len(ixtoword.keys())\n","    return textual_ids, textual_labels, ixtoword, wordtoix\n","  \n","  def get_label(self, index):\n","    if self.textual_labels[index] == 'positive':\n","      return 0\n","    return 1\n","   \n","  def get_text(self, index):\n","    sentence = self.textual_ids[index][:MAX_LEN]\n","    while len(sentence) < MAX_LEN:\n","      sentence.append(END)\n","    return sentence\n","    \n","  def __len__(self):\n","    return len(self.textual_ids)\n","  \n","  def __getitem__(self, index):\n","    text = torch.Tensor(self.get_text(index))\n","    text_len = len(self.textual_ids[index])\n","    lbl = torch.Tensor([self.get_label(index)])\n","\n","    return text, text_len, lbl\n","    \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"baD8lYAytdTV","colab_type":"text"},"source":["## Initialize the Dataloader\n","We initialize the training and testing dataloaders using the Dataset classes we create for both training and testing. Make sure you use the same vocabulary for both the datasets."]},{"cell_type":"code","metadata":{"id":"WCzNm8LDM5aT","colab_type":"code","outputId":"da43df1a-176f-4365-cf80-9e824c7e629c","executionInfo":{"status":"ok","timestamp":1575660699318,"user_tz":480,"elapsed":16230,"user":{"displayName":"Joe Fetsch","photoUrl":"","userId":"03543581272752113099"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["if(__name__=='__main__'):\n","  Ds = TextDataset(train_data, 'train')\n","  train_loader = torch.utils.data.DataLoader(Ds, batch_size=32, shuffle=True, num_workers=4, drop_last=True)\n","  test_Ds = TextDataset(test_data, 'test', wordtoix=Ds.wordtoix, ixtoword=Ds.ixtoword)\n","  test_loader = torch.utils.data.DataLoader(test_Ds, batch_size=1, shuffle=False, num_workers=4, drop_last=True)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Built dicts with len = 1469\n","  from source with 6920 reviews\n","  producing 6920 index vectors\n","  with 3310 positive reviews and 3610 negative ones\n","  at Threshold = 10\n","Built dicts with len = 1469\n","  from source with 1821 reviews\n","  producing 1821 index vectors\n","  with 912 positive reviews and 909 negative ones\n","  at Threshold = 10\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BRCFvjwDthiA","colab_type":"text"},"source":["## Build your Sequential Model\n","In the following we provide you the class to build your model. We provide some parameters, we expect you to use in the initialization of your sequential model."]},{"cell_type":"code","metadata":{"id":"2nc_HxbP6klI","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","\n","class RNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n","                 bidirectional, dropout, pad_idx):\n","        super().__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.output_dim = output_dim\n","        self.dropout = dropout\n","        self.vocab_size = vocab_size\n","        self.n_layers = n_layers\n","        self.pad_idx = pad_idx\n","        self.bidirectional = bidirectional\n","        self.embeds = nn.Embedding(vocab_size, embedding_dim, pad_idx)\n","        self.rnn = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_dim, num_layers = n_layers, dropout = dropout, bidirectional = bidirectional)\n","        self.linear = nn.Linear(hidden_dim*(2 if bidirectional else 1), output_dim)\n","        self.hidden = None\n","        self.c = None\n","        \n","    def init_hidden(self, batch_size): # causes issues with CUDA\n","        self.hidden = torch.rand(self.n_layers*(2 if self.bidirectional else 1), batch_size, self.hidden_dim).float()\n","        self.c = torch.rand(self.n_layers*(2 if self.bidirectional else 1), batch_size, self.hidden_dim).float()\n","        \n","    def forward(self, text, text_lengths):        \n","        # INPUTS\n","        #text = [MAX LEN, batch size]\n","        #text_lengths = [batch size]\n","        embeds = self.embeds(text.long())\n","        #embeds = [MAX_LEN, batch_size, embedding_dim]        \n","        embeds = nn.utils.rnn.pack_padded_sequence(embeds, text_lengths, enforce_sorted=False).float()\n","        embeds.to(device)\n","        \n","        # LSTM\n","        if self.hidden is None:\n","          lstm_out, (self.hidden, self.c) = self.rnn(embeds)\n","        else:\n","          lstm_out, (self.hidden, self.c) = self.rnn(embeds, (self.hidden, self.c))\n","        lstm_out = nn.utils.rnn.pad_packed_sequence(lstm_out, total_length=MAX_LEN)\n","        #hidden = [n_layers*num_directions, batch_size, hidden_dim]\n","        #c = [n_layers*num_directions, batch_size, hidden_dim]\n","        #lstm_out[0] = [MAX_LEN, batch_size, hidden_dim*num_directions]\n","        #lstm_out[1] = text_lengths\n","        \n","        \n","        # LINEAR, OUTPUT\n","        linear_input = lstm_out[0][0]\n","        y_pred = self.linear(linear_input)\n","        #y_pred = [batch_size,1]\n","        return y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WpjJP0746l2Y","colab_type":"code","colab":{}},"source":["# Hyperparameters for your model\n","# Feel Free to play around with these\n","# for getting optimal performance\n","\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 1\n","N_LAYERS = 3\n","BIDIRECTIONAL = True\n","DROPOUT = 0.6\n","PAD_IDX = 0\n","LEARNING_RATE = 2.5e-3\n","\n","model = RNN(INPUT_DIM, \n","            EMBEDDING_DIM, \n","            HIDDEN_DIM, \n","            OUTPUT_DIM, \n","            N_LAYERS, \n","            BIDIRECTIONAL, \n","            DROPOUT, \n","            PAD_IDX)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CA-UairGErap","colab_type":"code","outputId":"2a413e4a-ef2e-4927-fccf-20f58263b1c6","executionInfo":{"status":"ok","timestamp":1575660699320,"user_tz":480,"elapsed":16211,"user":{"displayName":"Joe Fetsch","photoUrl":"","userId":"03543581272752113099"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","if(__name__=='__main__'):\n","  print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["The model has 4,034,517 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CFemSvgdtyl6","colab_type":"text"},"source":["### Put your model on the GPU"]},{"cell_type":"code","metadata":{"id":"C1z_XinGE8z-","colab_type":"code","colab":{}},"source":["if(__name__=='__main__'):\n","  model = model.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NqWHgigKtrz5","colab_type":"text"},"source":["### Define your loss function and optimizer"]},{"cell_type":"code","metadata":{"id":"TmvK3l0cE4dD","colab_type":"code","colab":{}},"source":["import torch.optim as optim\n","\n","# Play around with different optimizers and loss functions\n","# for getting optimal performance\n","# For optimizers : https://pytorch.org/docs/stable/optim.html\n","# For loss functions : https://pytorch.org/docs/stable/nn.html#loss-functions\n","if(__name__=='__main__'):\n","  #optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n","  optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","  #optimizer = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9)\n","  criterion = nn.BCEWithLogitsLoss() \n","  criterion.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5zpJmsJiE-gq","colab_type":"code","colab":{}},"source":["def binary_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float() #convert into float for division \n","    acc = correct.sum() / len(correct)\n","    return acc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0DJc8Rxzt3JS","colab_type":"text"},"source":["## Train your Model"]},{"cell_type":"code","metadata":{"id":"LD-Jj2rUFOzr","colab_type":"code","colab":{}},"source":["import datetime\n","def train_model(model, num_epochs, data_loader):\n","  start = datetime.datetime.now()\n","  model.train()\n","  for epoch in range(10):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    for idx, (text, text_lens, label) in enumerate(data_loader):\n","        if(idx%100==0):\n","          print('Executed Step {} of Epoch {}'.format(idx, epoch))\n","        text = text.to(device)\n","        # text - [batch_len, MAX_LEN]\n","        text_lens = text_lens.to(device)\n","        # text - [batch_len]\n","        label = label.float()\n","        label = label.to(device)\n","                \n","        optimizer.zero_grad()\n","        text = text.permute(1, 0) # permute for sentence_len first for embedding\n","        predictions = model(text, text_lens)\n","        loss = criterion(predictions, label)\n","        acc = binary_accuracy(predictions, label)\n","\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        \n","        # allow backprop of loss calculations\n","        model.hidden.detach_()\n","        model.c.detach_()\n","        model.hidden = model.hidden.detach()\n","        model.c = model.c.detach()\n","        model.to(device)\n","    print('Training Loss Value of Epoch {} = {}'.format(epoch ,epoch_loss/len(train_loader)))\n","    print('Training Accuracy of Epoch {} = {}'.format(epoch ,epoch_acc/len(train_loader)))\n","    \n","    # if we are stuck with terrible accuracy, refresh the hidden layers\n","    #if epoch_acc/len(train_loader) < 0.54:\n","    #  global BATCH_SIZE\n","    #  print(\"Resetting hidden layers due to low accuracy\")\n","    #  model.init_hidden(BATCH_SIZE)\n","    #  model.to(device)\n","      \n","    end = datetime.datetime.now()\n","    print(end-start)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q-OJbZ72t6Yq","colab_type":"text"},"source":["## Evaluate your Model"]},{"cell_type":"code","metadata":{"id":"vTiiYDZIF--7","colab_type":"code","colab":{}},"source":["def evaluate(model, data_loader):\n","  model.eval()\n","  epoch_loss = 0\n","  epoch_acc = 0\n","  all_predictions = []\n","  for idx, (text, text_lens, label) in enumerate(data_loader):      \n","      if(idx%100==0):\n","        print('Executed Step {}'.format(idx))\n","      text = text.permute(1, 0)\n","      text = text.to(device)\n","      text_lens = text_lens.to(device)\n","      label = label.float()\n","      label = label.to(device)\n","      optimizer.zero_grad()\n","      \n","      predictions = model(text, text_lens)\n","      all_predictions.append(torch.round(torch.sigmoid(predictions)))\n","      loss = criterion(predictions, label)\n","      acc = binary_accuracy(predictions, label)\n","      epoch_loss += loss.item()\n","      epoch_acc += acc.item()\n","      \n","  print(epoch_loss/len(data_loader))\n","  print(epoch_acc/len(data_loader))\n","  predictions = torch.cat(all_predictions)\n","  return predictions"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FhNl48rew3KW","colab_type":"text"},"source":["## Training and Evaluation\n","\n","We first train your model using the training data. Feel free to play around with the number of epochs. We recommend **you write code to save your model** [(save/load model tutorial)](https://pytorch.org/tutorials/beginner/saving_loading_models.html) as colab connections are not permanent and it can get messy if you'll have to train your model again and again."]},{"cell_type":"code","metadata":{"id":"mmk-pkthw6xW","colab_type":"code","outputId":"36cbdc9b-2168-400b-afcb-01afbfd92426","executionInfo":{"status":"ok","timestamp":1575660941264,"user_tz":480,"elapsed":258136,"user":{"displayName":"Joe Fetsch","photoUrl":"","userId":"03543581272752113099"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["if(__name__=='__main__'):\n","  try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","  except:\n","    pass\n","  #model = torch.load('drive/My Drive/UIUC Classes/CS 447 Natural Language Processing/Assignments/HW2/model_74')\n","  train_model(model, 10, train_loader)\n","  torch.save(model, 'drive/My Drive/UIUC Classes/CS 447 Natural Language Processing/Assignments/HW2/model')"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","Executed Step 0 of Epoch 0\n","Executed Step 100 of Epoch 0\n","Executed Step 200 of Epoch 0\n","Training Loss Value of Epoch 0 = 0.6864623245265749\n","Training Accuracy of Epoch 0 = 0.5583043981481481\n","0:00:19.850455\n","Executed Step 0 of Epoch 1\n","Executed Step 100 of Epoch 1\n","Executed Step 200 of Epoch 1\n","Training Loss Value of Epoch 1 = 0.6058587493995825\n","Training Accuracy of Epoch 1 = 0.6822916666666666\n","0:00:39.383569\n","Executed Step 0 of Epoch 2\n","Executed Step 100 of Epoch 2\n","Executed Step 200 of Epoch 2\n","Training Loss Value of Epoch 2 = 0.48450061678886414\n","Training Accuracy of Epoch 2 = 0.7767650462962963\n","0:00:58.979595\n","Executed Step 0 of Epoch 3\n","Executed Step 100 of Epoch 3\n","Executed Step 200 of Epoch 3\n","Training Loss Value of Epoch 3 = 0.39308773574453815\n","Training Accuracy of Epoch 3 = 0.8256655092592593\n","0:01:18.472692\n","Executed Step 0 of Epoch 4\n","Executed Step 100 of Epoch 4\n","Executed Step 200 of Epoch 4\n","Training Loss Value of Epoch 4 = 0.31362068380608604\n","Training Accuracy of Epoch 4 = 0.8699363425925926\n","0:01:37.884237\n","Executed Step 0 of Epoch 5\n","Executed Step 100 of Epoch 5\n","Executed Step 200 of Epoch 5\n","Training Loss Value of Epoch 5 = 0.25039117152078283\n","Training Accuracy of Epoch 5 = 0.9030671296296297\n","0:01:57.320017\n","Executed Step 0 of Epoch 6\n","Executed Step 100 of Epoch 6\n","Executed Step 200 of Epoch 6\n","Training Loss Value of Epoch 6 = 0.19459151092019897\n","Training Accuracy of Epoch 6 = 0.9288194444444444\n","0:02:16.848039\n","Executed Step 0 of Epoch 7\n","Executed Step 100 of Epoch 7\n","Executed Step 200 of Epoch 7\n","Training Loss Value of Epoch 7 = 0.1723420151975006\n","Training Accuracy of Epoch 7 = 0.9392361111111112\n","0:02:36.373693\n","Executed Step 0 of Epoch 8\n","Executed Step 100 of Epoch 8\n","Executed Step 200 of Epoch 8\n","Training Loss Value of Epoch 8 = 0.13597955919491747\n","Training Accuracy of Epoch 8 = 0.9532696759259259\n","0:02:55.922999\n","Executed Step 0 of Epoch 9\n","Executed Step 100 of Epoch 9\n","Executed Step 200 of Epoch 9\n","Training Loss Value of Epoch 9 = 0.11871272844210681\n","Training Accuracy of Epoch 9 = 0.9613715277777778\n","0:03:15.484751\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"0h-EbXgz74Rw","colab_type":"text"},"source":["Now we will evaluate your model on the test set."]},{"cell_type":"code","metadata":{"id":"g4kssEOSxMU-","colab_type":"code","outputId":"d7a82792-6b74-405d-ba9b-293d5fffbbc7","executionInfo":{"status":"ok","timestamp":1575660958924,"user_tz":480,"elapsed":275789,"user":{"displayName":"Joe Fetsch","photoUrl":"","userId":"03543581272752113099"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["if(__name__=='__main__'):\n","  predictions = evaluate(model, test_loader)\n","  predictions = predictions.cpu().data.detach().numpy()\n","  assert(len(predictions)==len(test_data))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Executed Step 0\n","Executed Step 100\n","Executed Step 200\n","Executed Step 300\n","Executed Step 400\n","Executed Step 500\n","Executed Step 600\n","Executed Step 700\n","Executed Step 800\n","Executed Step 900\n","Executed Step 1000\n","Executed Step 1100\n","Executed Step 1200\n","Executed Step 1300\n","Executed Step 1400\n","Executed Step 1500\n","Executed Step 1600\n","Executed Step 1700\n","Executed Step 1800\n","0.8604894373900902\n","0.7644151565074135\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8WQAV6O2xHvS","colab_type":"text"},"source":["## Saving results for Submission\n","Saving your test results for submission. You will save the `result.txt` with your test data results. Make sure you do not **shuffle** the order of the `test_data` or the autograder will give you a bad score.\n","\n","You will submit the following files to the autograder on the gradescope :\n","\n","\n","1.   Your `result.txt` of test data results\n","2.   Your code of this notebook. You can do it by clicking `File`-> `Download .py` - make sure the name of the downloaded file is `assignment2.py`\n","\n"]},{"cell_type":"code","metadata":{"id":"abbbMNi8X_ai","colab_type":"code","outputId":"0fabf5fd-fb5a-41c3-d231-70ec2aca0b3a","executionInfo":{"status":"ok","timestamp":1575660959119,"user_tz":480,"elapsed":275979,"user":{"displayName":"Joe Fetsch","photoUrl":"","userId":"03543581272752113099"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["if(__name__=='__main__'):\n","  try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","  except:\n","    pass\n","  np.savetxt('drive/My Drive/UIUC Classes/CS 447 Natural Language Processing/Assignments/HW2/result.txt', predictions, delimiter=',')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]}]}